{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:34:32.743663Z",
     "start_time": "2024-01-11T08:34:32.692110400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import kineticstoolkit.lab as ktk\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "from markers_analysis import markers\n",
    "from markers_analysis import constants as consts\n",
    "from markers_analysis import subject as subj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:34:32.760348200Z",
     "start_time": "2024-01-11T08:34:32.749647600Z"
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "# %matplotlib qt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:34:32.793033600Z",
     "start_time": "2024-01-11T08:34:32.758093700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'markers_analysis.markers' from 'C:\\\\Users\\\\noamg\\\\OneDrive - post.bgu.ac.il\\\\Documents\\\\motor learning lab\\\\GitHub\\\\Noam-markers-analysis\\\\markers_analysis\\\\markers.py'>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import markers_analysis\n",
    "\n",
    "importlib.reload(markers_analysis)\n",
    "importlib.reload(consts)\n",
    "importlib.reload(subj)\n",
    "importlib.reload(markers)\n",
    "importlib.reload(markers_analysis.constants)\n",
    "importlib.reload(markers_analysis.subject)\n",
    "importlib.reload(markers_analysis.markers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject list and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:34:32.794276Z",
     "start_time": "2024-01-11T08:34:32.785968800Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = os.path.join('.', 'pkl')\n",
    "fig_path = os.path.join('.', 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:34:32.817318400Z",
     "start_time": "2024-01-11T08:34:32.791902700Z"
    }
   },
   "outputs": [],
   "source": [
    "basename = 'white_ball_hit '\n",
    "subject_id_list = ['007', '008', '009', '010', '011', '012', '013', '014']\n",
    "date_list = ['2023_11_20', '2023_11_20', '2023_11_20', '2023_12_03', '2023_12_03', '2023_12_03', '2023_12_14', '2023_12_14']\n",
    "\n",
    "# subject_id_list = ['007', '008', '009', '010', '011']\n",
    "# date_list = ['2023_11_20', '2023_11_20', '2023_11_20', '2023_12_03', '2023_12_03']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:34:32.813388100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning [C:\\Users\\noamg\\anaconda3\\envs\\ktk\\Lib\\site-packages\\kineticstoolkit\\filters.py:53] NaNs found in the signal. They have been interpolated before filtering, and then put back in the filtered data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Done with subject f007 file number 1: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 1 Backup 2023-12-02 21.33.35.c3d\n",
      "************* Done with subject f007 file number 2: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 2 Backup 2023-12-02 21.33.22.c3d\n",
      "************* Done with subject f007 file number 3: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 3 Backup 2023-12-02 21.33.11.c3d\n",
      "************* Done with subject f007 file number 4: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 4 Backup 2023-12-02 21.32.57.c3d\n",
      "************* Done with subject f007 file number 5: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 5 Backup 2023-12-02 21.32.46.c3d\n",
      "************* Done with subject f007 file number 6: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 6 Backup 2023-12-02 21.32.34.c3d\n",
      "************* Done with subject f007 file number 7: ..\\data\\subject_007\\2023_11_20\\markers\\white_ball_hit 7.c3d\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\pkl\\\\id007 markers.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 31\u001B[0m\n\u001B[0;32m     28\u001B[0m         subject_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(data_dict)\n\u001B[0;32m     29\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m************* Done with subject f\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubject_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minfo\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m file number \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubject_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilenum\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubject_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 31\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmarker_file_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     32\u001B[0m         pickle\u001B[38;5;241m.\u001B[39mdump(subject_data, file)\n\u001B[0;32m     34\u001B[0m all_data\u001B[38;5;241m.\u001B[39mappend(subject_data)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ktk\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[1;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '.\\\\pkl\\\\id007 markers.pkl'"
     ]
    }
   ],
   "source": [
    "interconnections = markers.get_interconnections()\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for subject_id,date in zip(subject_id_list,date_list):\n",
    "    marker_file_name = os.path.join(results_path, f'id{subject_id} markers.pkl')\n",
    "\n",
    "    if os.path.exists(marker_file_name):\n",
    "        # File exists, unpickle the data\n",
    "        with open(marker_file_name, 'rb') as file:\n",
    "            subject_data = pickle.load(file)\n",
    "    else:\n",
    "        # Process the file\n",
    "        subject_data = {}\n",
    "\n",
    "        info = {}\n",
    "        info['id'] = subject_id\n",
    "        info['date'] = date\n",
    "        info['basename'] = basename\n",
    "        subject_data['info'] = info\n",
    "        subject_data['data'] = []\n",
    "\n",
    "        for filenum,filename in subj.find_marker_data_files(date, subject_id):\n",
    "            data_dict = subj.load_marker_data_file(filename, interconnections)\n",
    "\n",
    "            data_dict['filenum'] = filenum\n",
    "            data_dict['filename'] = filename\n",
    "            subject_data['data'].append(data_dict)\n",
    "            print(f\"************* Done with subject f{subject_data['info']['id']} file number {subject_data['data'][-1]['filenum']}: {subject_data['data'][-1]['filename']}\")\n",
    "\n",
    "        with open(marker_file_name, 'wb') as file:\n",
    "            pickle.dump(subject_data, file)\n",
    "\n",
    "    all_data.append(subject_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut data files\n",
    "\n",
    "This is slow so it is disabled and the data is loaded from the pickled file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:35:31.170400500Z",
     "start_time": "2024-01-11T08:35:31.165892300Z"
    }
   },
   "outputs": [],
   "source": [
    "all_cut_data = []\n",
    "\n",
    "for subject_data in all_data:\n",
    "    subject_id = subject_data['info']['id']\n",
    "    date = subject_data['info']['date']\n",
    "\n",
    "    table_cut_filename = os.path.join(results_path, f\"id{subject_id} cut.pkl\")\n",
    "    marker_cut_filename = os.path.join(results_path, f\"id{subject_id} table markers cut.pkl\")\n",
    "\n",
    "    with open(table_cut_filename, 'rb') as file:\n",
    "        table_data = pickle.load(file)\n",
    "    removed_table_shots = False\n",
    "\n",
    "    if os.path.exists(marker_cut_filename):\n",
    "        # File exists, unpickle the data\n",
    "        with open(marker_cut_filename, 'rb') as file:\n",
    "            shots_data = pickle.load(file)\n",
    "    else:\n",
    "        shots_data = {}\n",
    "        shots_data['info'] = subject_data['info']\n",
    "        shots_data['shots'] = []\n",
    "\n",
    "        for data_dict in subject_data['data']:\n",
    "            file_data = data_dict[\"markers\"]\n",
    "\n",
    "            filenum = data_dict[\"filenum\"]\n",
    "            frames = data_dict[\"frames\"]\n",
    "            euler = data_dict[\"euler\"]\n",
    "            euler_vels = data_dict[\"euler_vels\"]\n",
    "\n",
    "            ts_rename = [frames, euler, euler_vels]\n",
    "            ts_names = ['frames', 'angles', 'vels']\n",
    "            for ts, name in zip(ts_rename, ts_names):\n",
    "                old_names = list(ts.data.keys())\n",
    "                new_names = [f'{name}_{on}' for on in old_names]\n",
    "                for o,n in zip(old_names, new_names):\n",
    "                    ts.rename_data(o, n, in_place=True)\n",
    "                file_data.merge(ts, in_place=True)\n",
    "\n",
    "            file_shots = [s for s in table_data['shots'] if s['filenum'] == filenum]\n",
    "\n",
    "            indices_to_remove = []\n",
    "            time = np.arange(start=-1, stop=1, step=0.01)\n",
    "            for s in file_shots:\n",
    "                # If we've run out of marker file (why??)\n",
    "                if file_data.time[-1] < s['zero_time']-1:\n",
    "                    indices_to_remove.append(table_data['shots'].index(s))\n",
    "                    continue\n",
    "\n",
    "                start_time = np.max([file_data.time[0], s['zero_time']-1])\n",
    "                end_time = np.min([file_data.time[-1], s['zero_time']+1])\n",
    "\n",
    "                shot_ts = file_data.get_ts_between_times(start_time, end_time, inclusive=True)\n",
    "                shot_ts.shift(-s['zero_time'], in_place=True)\n",
    "\n",
    "                shot_ts.resample(time, in_place=True)\n",
    "\n",
    "                shot_data = {}\n",
    "                shot_data['filenum'] = filenum\n",
    "                shot_data['shotnum'] = s['shotnum']\n",
    "                shot_data['start_time'] = start_time\n",
    "                shot_data['zero_time'] = s['zero_time']\n",
    "                shot_data['end_time'] = end_time\n",
    "                \n",
    "                shot_data['interconnections'] = data_dict[\"interconnections\"]\n",
    "                shot_data['global_transform'] = data_dict[\"global_transform\"]\n",
    "\n",
    "                shot_data['ts'] = shot_ts\n",
    "\n",
    "                shots_data['shots'].append(shot_data)\n",
    "            \n",
    "            for i in reversed(indices_to_remove):\n",
    "                del table_data['shots'][i]\n",
    "            if indices_to_remove:\n",
    "                removed_table_shots = True\n",
    "\n",
    "        with open(marker_cut_filename, 'wb') as file:\n",
    "            pickle.dump(shots_data, file)\n",
    "\n",
    "    if removed_table_shots:\n",
    "        with open(table_cut_filename, 'wb') as file:\n",
    "            pickle.dump(table_data, file)\n",
    "\n",
    "    all_cut_data.append({'marker': shots_data, 'table': table_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot position and velocity \"learning curves\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get successes for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.174549800Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_success_list = []\n",
    "for subject_data in all_cut_data:\n",
    "    marker_data = subject_data['marker']\n",
    "    table_data = subject_data['table']\n",
    "\n",
    "    hits = subj.is_hit(table_data['shots'])\n",
    "    shots_good = [m for i,m in enumerate(marker_data['shots']) if hits[i]]\n",
    "\n",
    "    df_list = []\n",
    "    for shot in shots_good:\n",
    "        ts = shot['ts']\n",
    "        success_test_index = ts.get_index_before_time(0)\n",
    "        shot_df = ts.to_dataframe().iloc[[success_test_index],:]\n",
    "\n",
    "        df_list.append(shot_df)\n",
    "\n",
    "    success_df = pd.concat(df_list, axis=0, ignore_index = True)\n",
    "    # Take robust mean of this subject's movements\n",
    "    success_df.fillna(np.inf)\n",
    "    mean_success = success_df.apply(scipy.stats.trim_mean, proportiontocut=0.1)\n",
    "    success_df.replace(np.inf, np.nan)\n",
    "\n",
    "    mean_success_list.append(mean_success)\n",
    "\n",
    "mean_success_df = pd.concat(mean_success_list, axis=1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.183648600Z"
    }
   },
   "outputs": [],
   "source": [
    "# import statsmodels.api as sem_dfs\n",
    "\n",
    "\n",
    "\n",
    "mean_success_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a dataframe of the time zero locations and velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.190805700Z"
    }
   },
   "outputs": [],
   "source": [
    "time_0_data_list = []\n",
    "for subject_data in all_cut_data:\n",
    "    marker_data = subject_data['marker']\n",
    "    table_data = subject_data['table']\n",
    "    \n",
    "    plot_list = []\n",
    "    for shot in marker_data['shots']:\n",
    "        ts = shot['ts']\n",
    "        plot_ts = ts.get_subset(['R_SAE', 'R_ELB', 'vels_R_Arm', 'vels_R_Forearm'])\n",
    "        plot_index = plot_ts.get_index_before_time(0)\n",
    "\n",
    "        shot_df = plot_ts.to_dataframe().iloc[[plot_index], :]\n",
    "        plot_list.append(shot_df)\n",
    "\n",
    "    plot_df = pd.concat(plot_list, ignore_index=True)\n",
    "\n",
    "    time_0_data_list.append(plot_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now remove outliers from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.197952700Z"
    }
   },
   "outputs": [],
   "source": [
    "for plot_df in time_0_data_list:\n",
    "\n",
    "    # Loop over the columns of the original data frame\n",
    "    for col_name, col_series in plot_df.items():\n",
    "        # Remove NaNs from the column\n",
    "        col_series = col_series.dropna()\n",
    "        \n",
    "        col_median = col_series.median()\n",
    "        col_mad = (col_series - col_median).abs().median()\n",
    "\n",
    "\n",
    "        # Define the lower and upper bounds for outliers\n",
    "        lower_bound = col_median - 3 * 1.5 * col_mad\n",
    "        upper_bound = col_median + 3 * 1.5 * col_mad\n",
    "        # Replace outliers with NaNs\n",
    "\n",
    "        col_series = col_series.mask((col_series < lower_bound) | (col_series > upper_bound), np.nan)\n",
    "        # Add the column to the new data frame\n",
    "        plot_df[col_name] = col_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now get the distance from the time 0 to the average of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.206640900Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# Function to interpolate NaN values in a column\n",
    "def interpolate_column(column):\n",
    "    indices = np.arange(len(column))\n",
    "    mask = np.isnan(column)\n",
    "    column[mask] = np.interp(indices[mask], indices[~mask], column[~mask])\n",
    "    return column\n",
    "\n",
    "def dist_to_success(df, mean_success, name, dims=[0,1,2]):\n",
    "    names = [f'{name}[{i}]' for i in dims]\n",
    "    delta = np.array(\n",
    "        [df[n] - mean_success[n] for n in names]\n",
    "    ).T\n",
    "    np.apply_along_axis(interpolate_column, axis=0, arr=delta)\n",
    "\n",
    "    return np.linalg.norm(delta, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.212674900Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_joint_velocity_angles_dict = {\n",
    "    'Pelvis': 2, 'Thorax': 1, 'Arm': 0, 'Forearm': 0, 'Hand': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.216323400Z"
    }
   },
   "outputs": [],
   "source": [
    "time_0_dist_list = []\n",
    "\n",
    "for plot_df, (i,mean_success) in zip(time_0_data_list, mean_success_df.iterrows()):\n",
    "    \n",
    "    dist_df = pd.DataFrame()\n",
    "\n",
    "    for j in ['R_SAE', 'R_ELB']:\n",
    "        dist_df[j] = dist_to_success(plot_df, mean_success, j)\n",
    "\n",
    "    for j in ['vels_R_Arm', 'vels_R_Forearm']:\n",
    "        dist_df[j] = dist_to_success(plot_df, mean_success, j, dims=[0])\n",
    "    \n",
    "    time_0_dist_list.append(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.226026900Z"
    }
   },
   "outputs": [],
   "source": [
    "upper_arm = vel_dfs['Upper arm'][zero_index]\n",
    "upper_arm_success = upper_arm[hits].mean()\n",
    "\n",
    "forearm = vel_dfs['Forearm'][zero_index]\n",
    "forearm_success = forearm[hits].mean()\n",
    "\n",
    "dist_df['vels_R_Arm'] = np.abs(upper_arm - upper_arm_success)\n",
    "dist_df['vels_R_Forearm'] = np.abs(forearm - forearm_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.233020700Z"
    }
   },
   "outputs": [],
   "source": [
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T08:35:31.239255600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# Flatten the axes array if needed\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_in_axes = {'R_SAE': 0, 'R_ELB': 0, 'vels_R_Arm': 1, 'vels_R_Forearm': 1}\n",
    "labels = {'R_SAE': 'Shoulder', 'R_ELB': 'Elbow', 'vels_R_Arm': 'Upper arm', 'vels_R_Forearm': 'Forearm'}\n",
    "scale = {'R_SAE': 100, 'R_ELB': 100, 'vels_R_Arm': 1, 'vels_R_Forearm': 1}\n",
    "\n",
    "\n",
    "# Iterate through columns and plot\n",
    "for col in dist_df.columns:\n",
    "    ax = axes[plot_in_axes[col]]\n",
    "    ax.plot(dist_df.index, dist_df[col]*scale[col], label=labels[col], linewidth=2)\n",
    "\n",
    "axes[0].axvspan(xmin=0, xmax=15, color='#D2B48C', alpha=0.3, label='Baseline')\n",
    "axes[1].axvspan(xmin=0, xmax=15, color='#D2B48C', alpha=0.3, label='Baseline')\n",
    "\n",
    "axes[0].set_xlim(0, 200)\n",
    "axes[0].legend(loc='upper right', fontsize=12)\n",
    "axes[0].set_title('Joint position')\n",
    "axes[0].set_xlabel('Shot')\n",
    "axes[0].set_ylabel('Error (cm)')\n",
    "\n",
    "axes[1].set_xlim(0, 200)\n",
    "axes[1].legend(loc='upper right', fontsize=12)\n",
    "axes[1].set_title('Angular velocity')\n",
    "axes[1].set_xlabel('Shot')\n",
    "axes[1].set_ylabel('Error (deg/sec)')\n",
    "\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_savename = os.path.join(fig_path, f\"joint learning {subject_id}.png\")\n",
    "plt.savefig(fig_savename, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a883bdc580ed6e14c7ef4be3c7b042c6a2681728cd2297b4789711a793e6cf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
